---
title: "<strong>Functional Programming</strong>"
author: "andrés castro araújo"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output: 
  html_document: 
    theme: paper
    toc: yes
    toc_float:
      collapsed: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = "center")
```

```{css, echo=FALSE}
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    color: #828282;
    border-left: 10px solid #EEE;
}

body {
    font-size: 14px;
}
```

>All of this is taken from Hadley Wickham's [__Advanced R__](https://adv-r.hadley.nz). CRC Press, 2019.

****

## Introduction

R lends itself to a _functional style_ of programming. 
 
>Functional languages have __first-class functions__, functions that behave _like any other data structure._ In R, this means that you can do many of the things with a function that you can do with a vector: you can assign them to variables, store them in lists, pass them as arguments to other functions, create them inside functions, and even return them as the result of a function.

Many functional languages require that functions are __pure__ (i.e. that every input results in the same outputs). This is obviously not the case in R (e.g. `runif()`, `read.csv()`). So even though R isn't strictly speaking a functional language, it certainly resembles one.

>It’s hard to describe exactly what a functional _style_ is, but generally I think it means decomposing a big problem into smaller pieces, then solving each piece with a function or combination of functions. When using a functional style, you strive to decompose components of the problem into isolated functions that operate independently. Each function taken by itself is simple and straightforward to understand; complexity is handled by composing functions in various ways.

This notebook in particular is about __higher-order functions__. These functions can be categorized depending on their inputs and outputs, in accordance with this handy two-by-two table:

```{r, echo=FALSE, out.width="300px"}
knitr::include_graphics(
"https://d33wubrfki0l68.cloudfront.net/1dff819e743f280bbab1c55f8f063e60b6a0d2fb/2269e/diagrams/fp.png"
)
```

## Functionals

>A __functional__ is a function that takes a function as an input and returns a vector as output. Here’s a simple functional: it calls the function provided as input with 100 random uniform numbers.

```{r}
randomize <- function(f) f(runif(100))
randomize(mean)
randomize(sum)
```

>The chances are that you've already used a functional. You might have used for-loop replacements like base R's `lapply()`, `apply()`, and `tapply()`; or purrr's `map()`; or maybe you've used a mathematical functional like `integrate()` or `optim()`.

```{r}
integrate(dnorm, lower = 0, upper = Inf)
```

### Map

```{r, message=FALSE}
library(tidyverse)
```

The most important functional is ___`purrr::map()`___ (equivalent to `base::lapply()`): it takes a vector and a function, calls the function once for each element of the vector, and returns the results in a list.

```{r}
map(1:3, function(x) x^2)  ## using anonymous or lambda function
```

The name "map" comes from mathematics, where "an operation that associates each element of a given set with one or more elements of a second set".

```{r, echo=FALSE, out.width="260px"}
knitr::include_graphics(
"https://d33wubrfki0l68.cloudfront.net/f0494d020aa517ae7b1011cea4c4a9f21702df8b/2577b/diagrams/functionals/map.png"
)
```

The inner workings of `map()` are pretty much what we can expect from a for loop:

```{r, eval=FALSE}
output <- vector("list", length(x))
for (i in seq_along(x)) {
  output[[i]] <- f(x[[i]])
}
```

There are many variants to `map()` that can be identified via conventional suffixes like `_dbl` to specify the type of _atomic vector_ we want to receive as output.

```{r}
map_chr(mtcars, is.numeric)  ## remember that data frames are lists of lists.
map_dbl(mtcars, max) %>% 
  map_dbl(round, digits = 1) ## using dot-dot-dot 
```

The `map()` family is different from `lapply()` (and other `base` variants) because it supports some shortcuts.

For example, the "twiddle" syntax (~):

```{r}
identical(
  map_dbl(mtcars, ~ length(unique(..1))),          ## twiddle syntax
  map_dbl(mtcars, function(x) length(unique(x)))   ## alternative
  )

map(1:3, ~ rnorm(10)) %>% str()                         ## twiddle syntax
map(1:3, function(x) rnorm(10)) %>% str()               ## alternative
replicate(n = 3, rnorm(10), simplify = FALSE) %>% str() ## other alternative
```

_Varying a different argument_

>So far the first argument to `map()` has always become the first argument to the function. But what happens if the first argument should be constant, and you want to vary a different argument? How do you get the result in this picture?

```{r, echo=FALSE, out.width="300px"}
knitr::include_graphics(
"https://d33wubrfki0l68.cloudfront.net/6d0b927ba5266f886cc721ae090afcc5e872a748/f8636/diagrams/functionals/map-arg-flipped.png"
)
```

```{r}
draws <- rcauchy(1000)
trims <- c(0, 0.1, 0.2, 0.5)

map_dbl(trims, ~ mean(draws, trim = ..1))
map_dbl(trims, function(input) mean(x = draws, trim = input))
map_dbl(trims, mean, x = draws) ## using R's flexible argument matching
```

__Piping multiple `purrr` functions__

```{r}
base::split(mtcars, mtcars$cyl) %>% ## split data frame into three, by cyl
  map(function(df) lm(mpg ~ wt, data = df)) %>% ## fit linear model three times
  map(coefficients) %>% ## access coefficients
  map_dbl(pluck, "wt")  ## extract slope

## Loop alternative
slopes <- vector("double", 3)
for (i in seq_along(slopes)) {
  model <- lm(mpg ~ wt, data = base::split(mtcars, mtcars$cyl)[[i]])
  slopes[[i]] <- coef(model)[["wt"]]
}
slopes
```

___Exercises.___

>`map(1:3, ~ runif(2))` is a useful pattern for generating random numbers, but `map(1:3, runif(2))` is not. Why not? Can you explain why it returns the result that it does?

`map()` uses `as_mapper()` to turn it's second argument into a function that then gets applied to each element in the first argument. 

In the first case, the twiddle syntax succesfully turns `~ runif(2)` into an anonymous function that then gets applied one time for every iteration.

In the second case, `as_mapper()` fails to turn `runif(2)` into a function. If the second argument is a _character vector_, _numeric vector_, or _list_, then `as_mapper()` turns this into an extractor function that uses `purr::pluck()` to extract elements.

```{r}
as_mapper(~runif(2))
as_mapper(runif(2))
```

This is how it works:

```{r}
identical(map(1:3, 1), map(1:3, pluck, 1))
```

>Use the appropriate `map()` function to:

```{r}
# 1. Compute the standard deviation of every column in a numeric data frame.
map_dbl(mtcars, sd)

# 2. Compute the standard deviation of every numeric column in a mixed data frame.
index <- map_lgl(iris, is.numeric)
map_dbl(iris[index], sd)

# Compute the number of levels for every factor in a data frame.
map(iris[!index], attr, which = "levels")
```

>The following code simulates the performance of a t-test for non-normal data. Extract the p-value from each test, then visualise.

```{r, fig.height=3, fig.width=6}
trials <- map(1:1000, ~ t.test(rpois(10, 10), rpois(7, 10)))

tibble(p = map_dbl(trials, "p.value")) %>% 
  ggplot(aes(p)) + geom_histogram(binwidth = 0.05, color = "black")
```

>Use map() to fit linear models to the mtcars dataset using the formulas stored in this list:

```{r}
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

formulas %>% 
  map(~ lm(..1, data = mtcars)) %>% 
  map(coefficients)
```

>Fit the model `mpg ~ disp` to each of the bootstrap replicates of mtcars in the list below, then extract the $R^2$ of the model fit.

```{r}
bootstrap <- function(df) {
  df[sample(nrow(df), replace = TRUE), , drop = FALSE]
}

bootstraps <- map(1:10, ~ bootstrap(mtcars))

bootstraps %>% 
  map(function(df) lm(mpg ~ disp, data = df)) %>% 
  map(summary) %>% 
  map_dbl("r.squared") ## extractor 
```

### Varieties of Map

- Output same type as input with `modify()`

- Iterate over two inputs with `map2()`

- Iterate with an index using `imap()`

- Return nothing with `walk()`

- Iterate over any number of inputs with `pmap()`

```{r, echo=FALSE}
tibble(
  ` ` = c("One argument", "Two arguments", "One argument + index", "N arguments"),
  List = c("`map()`", "`map2()`", "`imap()`", "`pmap()`"),
  Atomic = c("`map_lgl(), ...`", "`map2_lgl(), ...`", "`imap_lgl(), ...`", "`pmap_lgl(), ...`"),
  `Same Type` = c("`modify()`", "`modify2()`", "`imodify()`", ""),
  Nothing = c("`walk()`", "`walk2()`", "`iwalk()`", "`pwalk()`")
  ) %>% 
  kableExtra::kable() %>% 
  kableExtra::kable_styling(bootstrap_options = "bordered")
```



__Same type of output as input: `modify()`__

```{r}
df <- data.frame(
  x = 1:3,
  y = 6:4
)

map(df, scale) %>% str()  ## returns a list
modify(df, scale)         ## returns a data.frame
```

__Two inputs: `map2()` and friends__

```{r, error=TRUE}
## Example: finding a weighted mean
x <- map(1:5, ~ rnorm(20))
wt <- map(1:5, ~ runif(20))

weighted.mean(x[[1]], wt[[1]]) ## first element of each list

## Passing wt as an additional argument doesn’t work because arguments
## after .f are not transformed:
map(x, weighted.mean, w = wt) 
```

For this we use the `map2()` variety, which is vectorised over two arguments. This means both `..1` and `..2` are varied in each call to `weighted.mean`

```{r}
x[[3]][[3]] <- NA
map2_dbl(x, wt, weighted.mean)
map2_dbl(x, wt, weighted.mean, na.rm = TRUE)
```

```{r, echo=FALSE, out.width="350px"}
knitr::include_graphics(
"https://d33wubrfki0l68.cloudfront.net/7a545699ff7069a98329fcfbe6e42b734507eb16/211a5/diagrams/functionals/map2-arg.png"
)
```

__No outputs: `walk()` and friends__

>Some functions are called primarily for their side-effects (e.g. `cat()`, `write.csv()`, or `ggsave()`) and it doesn’t make sense to capture their results.

```{r}
eat_fruit <- function(x) {
  cat("Eat some more ", x, "!\n", sep = "")
}

sample(fruit, size = 5) %>% 
  map(eat_fruit)
```

To prevent `NULL` from showing up, we use `walk()`, which returns the input values through `invisible()`.

```{r}
sample(fruit, size = 5) %>% 
  walk(eat_fruit)

walk(1:5, ~ rnorm(10))
```

A very common use case for this pattern is saving some object to disk.

```{r}
temp <- tempfile()
dir.create(temp)

species <- split(iris, iris$Species)  ## list of three
paths <- file.path(temp, paste0("species-", names(species), ".rds"))
walk2(species, paths, write_rds)

list.files(temp)
read_rds(paste0(temp, "/species-setosa.rds")) %>% head()
```

__Iterating over values and indices__

>There are three basic ways to loop over a vector with a for loop:

- Loop over the elements: `for (x in xs)`

- Loop over the numeric indices: `for (i in seq_along(xs))`

- Loop over the names: `for (nm in names(xs))`

>The first form is analogous to the map() family. The second and third forms are equivalent to the imap() family which allows you to iterate over the values and the indices of a vector in parallel.

>`imap()` is like `map2()` in the sense that your `.f` gets called with two arguments, but here both are derived from the vector. `imap(x, f)` is equivalent to `map2(x, names(x), f)` if `x` has names, and `map2(x, seq_along(x), f)` if it does not.

```{r}
## example: constructing labels
imap_chr(iris, ~ paste0("The first value in ", ..2, " is ", ..1[[1]]))
```

__Any number of inputs: `pmap()` and friends__

`pmap()` functions avoid the need to have `map3()`, `map4()`, `map5()`, etc.

>There’s a simple equivalence between `map2()` and `pmap()`: `map2(x, y, f)` is the same as `pmap(list(x, y), f)`.

For example:

```{r, echo=FALSE, out.width="300px"}
identical(
  pmap_dbl(list(x, wt), weighted.mean),
  map2_dbl(x, wt, weighted.mean)
  )
```

```{r, echo=FALSE, out.width="350px"}
knitr::include_graphics(
"https://d33wubrfki0l68.cloudfront.net/2eb2eefe34ad6d114da2a22df42deac8511b4788/5a538/diagrams/functionals/pmap-arg.png"
)
```

>A big difference between `pmap()` and the other map functions is that _`pmap()` gives you much finer control over argument matching because you can name the components of the list._

```{r}
draws <- rcauchy(1000)
trims <- c(0, 0.1, 0.2, 0.5)

identical(
  map_dbl(trims, function(input) mean(x = draws, trim = input)),
  pmap_dbl(list(trim = trims), mean, x = draws)
  )
```

Note that name matching makes `pmap()` very easy to work with data frames. 

For example:

```{r}
params <- data.frame(       ## Here, the column names
  n = c(1, 3, 5),           ## are critical!
  mean = c(0, 10, 1000),
  sd = c(1, 10, 100)
)

params

pmap(params, rnorm)
```

___Exercises___

>Rewrite the following code to use `iwalk()` instead of `walk2()`. What are the advantages and disadvantages?

```{r}
temp <- tempfile()
dir.create(temp)

cyls <- split(mtcars, mtcars$cyl)
paths <- file.path(temp, paste0("cyl-", names(cyls), ".csv"))

## Original
walk2(cyls, paths, write.csv)
list.files(temp)

## Solution
names(cyls) <- paths      ## Disadvantage: names are overwritten
iwalk(cyls, ~ write.csv)  ## Advantage: code is slightly shorter
list.files(temp)
```

### Reduce and Accumulate

>After the map family, the next most important family of functions is the reduce family. This family is much smaller, with only two main variants, and is used less commonly, but it's a powerful idea, gives us the opportunity to discuss some useful algebra, and powers the map-reduce framework frequently used for processing very large datasets.

>[`reduce()`](https://purrr.tidyverse.org/reference/reduce.html) takes a vector of length `n` and produces a vector of length `1` by calling a function with a pair of values at a time: `reduce(1:4, f)` is equivalent to `f(f(f(1, 2), 3), 4)`.

```{r, echo=FALSE, out.width="300px"}
knitr::include_graphics(
"https://d33wubrfki0l68.cloudfront.net/9c239e1227c69b7a2c9c2df234c21f3e1c74dd57/eec0e/diagrams/functionals/reduce.png"
)
```

For example, suppose we have 4 lists that contain fruit, and we want to figure out how what are the fruits common to _all_ of them. To do this, we have to call intersect five times (each time using the results of the previous call).

```{r}
fruit_lists <- map(1:4, ~sample(fruit, 35))

## This...
fruit_lists[[1]] %>% 
  intersect(fruit_lists[[2]]) %>% 
  intersect(fruit_lists[[3]]) %>% 
  intersect(fruit_lists[[4]]) 

## ...is equivalent to this
reduce(fruit_lists, intersect)
```

`accumulate()` is a variant of `reduce()` that returns all the intermediate results as well.

```{r}
accumulate(fruit_lists, intersect)
```

__Note.__ These two functions behave a lot like `sum()` and `cumsum()`.

```{r}
x <- c(4, 3, 10, 12)
reduce(x, `+`) ## sum()
accumulate(x, `+`) ## cumsum()
```

Finally, the `.init` argument allows you to supply an initial element to these functions.

```{r, echo=FALSE, out.width="300px"}
knitr::include_graphics(
"https://d33wubrfki0l68.cloudfront.net/b5835b80325b22f9460992f7bc9de5e0cf56de2c/27994/diagrams/functionals/reduce-init.png"
)
```

>So if we call ``reduce(1, `+`, init)`` the result will be `1 + init`.

```{r}
reduce(1:2, `+`, .init = 1)
```

This argument can become important if you are not sure what you're input might look like, and you want to avoid an error. In mathematics, we refer to the `.init` argument as the [identity element](https://en.wikipedia.org/wiki/Identity_element): "a special type of element of a set with respect to a binary operation on that set, which leaves any element of the set unchanged when combined with it."

```{r}
sum(integer())
prod(integer())
```

>If you’re using `reduce()` in a function, you should always supply `.init`. Think carefully about what your function should return when you pass a vector of length `0` or `1`, and make sure to test your implementation.

___Exercises (outside the book)___

>Implement Pascal's triangle using `accumulate()`

```{r}
## 1:5 gets passed to ... and never gets evaluated
accumulate(1:5, function(x, ...) c(0, x) + c(x, 0), .init = 1)
```


___Map-reduce___

>You might have heard of map-reduce, the idea that powers technology like Hadoop. Now you can see how simple and powerful the underlying idea is: map-reduce is a map combined with a reduce. The difference for large data is that the data is spread over multiple computers. Each computer performs the map on the data that it has, then it sends the result to back to a coordinator which reduces the individual results back to a single result.
>
>As a simple example, imagine computing the mean of a very large vector, so large that it has to be split over multiple computers. You could ask each computer to calculate the sum and the length, and then return those to the coordinator which computes the overall mean by dividing the total sum by the total length.

### Predicates

>A __predicate__ is a function that returns a single `TRUE` or `FALSE`, like `is.character()`, `is.null()`, or `all()`, and we say a predicate _matches_ a vector if it returns `TRUE.`

The `purrr` package has three pairs of useful functions that apply a predicate to each element in a vector.

1. _`some(.x, .p)` returns TRUE if any element matches; `every(.x, .p)` returns TRUE if all elements match._

    _These are equivalent to `any(map_lgl(.x, .p))` and `all(map_lgl(.x, .p))`, except that they terminate early: `some()` returns `TRUE` when it sees the first `TRUE`, and `every()` returns `FALSE` when it sees the first `FALSE.`_

2. _`detect(.x, .p)` returns the_ value _of the first match; `detect_index(.x, .p)` returns the_ location _of the first match._

3. _`keep(.x, .p)`_ keeps _all matching elements; `discard(.x, .p)`_ drops _all matching elements._

```{r}
df <- data.frame(
  x = 1:3,
  y = c("a", "b", "c")  ## stringsAsFactors = TRUE by default
  )

every(df, is.factor)
some(df, is.factor)

detect(df, is.integer)
detect_index(df, is.factor)

keep(df, is.factor) %>% str()
discard(df, is.factor) %>% str()
```

Note the following:

>`map()` and `modify()` come in variants that also take predicate functions, transforming only the elements of `.x` where `.p` is `TRUE`.

```{r}
map_if(df, is.numeric, mean) %>% str()
modify_if(df, is.numeric, max) %>% str()

## this will drop the factor, and avoid recycling
keep(df, is.numeric) %>% map(mean) %>% str()
```

___Exercises___

>Implement the `span()` function from Haskell: given a list `x` and a predicate function `f`, `span(x, f)` returns the location of the longest sequential run of elements where the predicate is true. (Hint: you might find `rle()` helpful.)

```{r}
span <- function(x, f) {
  ## this makes "values" and "lengths" available
  list2env(rle(map_lgl(x, f)), envir = rlang::caller_env())
  if (!any(values)) {
    return(integer())
  }
  index <- which(lengths == max(lengths[values]))[[1]] ## lgl sub
  first <- cumsum(lengths)[index] - (lengths[index] - 1) 
  return(first:(first + lengths[index] - 1))
}

span(iris, is.factor)
span(iris, is.numeric)
```

>Implement `arg_max()`. It should take a function and a vector of inputs, and return the Also implement the matching `arg_min()` function.

```{r}
arg_max <- function(x, f) {
  output <- map_dbl(x, f)
  x[output == max(output)] ## lgl subsetting
}

arg_min <- function(x, f) {
  output <- map_dbl(x, f)
  x[output == min(output)] ## lgl subsetting
}
```

### Extra

__Math__

>Functionals are very common in mathematics. The limit, the maximum, the roots (the set of points where $f(x) = 0$), and the definite integral are all functionals: given a function, they return a single number (or vector of numbers). At first glance, these functions don’t seem to fit in with the theme of eliminating loops, but if you dig deeper you’ll find out that they are all implemented using an algorithm that involves iteration.

- `integrate()` finds the area under the curve defined by `f()`

- `uniroot()` finds where `f()` hits zero

- `optimise()` finds the location of the lowest (or highest) value of `f()`

```{r, echo=FALSE, fig.width=5, fig.height=3}
ggplot(data.frame(x = c(pi/2, 2*pi)), aes(x)) + 
  stat_function(fun = sin) +
  labs(y = "f(x)")
```

```{r}
integrate(sin, lower = 2/pi, upper = 2*pi)
uniroot(sin, lower = 2/pi, upper = 2*pi)
optimise(sin, lower = 2/pi, upper = 2*pi)
optimise(sin, lower = 2/pi, upper = 2*pi, maximum = TRUE)
```

__Matrices and arrays__

The `map()` family of functions is designed to work with one-dimensional vectors. If we wish to work with n-dimensional vectors, we use `base::apply()`. It has four arguments:

- `X`, _the matrix or array to summarise._

- `MARGIN`, _an integer vector giving the dimensions to summarise over, 1 = rows, 2 = columns, etc. (The argument name comes from thinking about the margins of a joint distribution.)_

- `FUN`, _a summary function._

- `...` _other arguments passed on to `FUN`._

```{r}
A <- array(1:24, dim = c(4, 3, 2))
A
apply(A, MARGIN = 1, mean)
apply(A, MARGIN = 2, mean)
apply(A, MARGIN = 3, mean)
```

___Exercises___

>Challenge: read about the [fixed point algorithm](https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-12.html#%25_idx_1096). Complete the exercises using R.

A number $x$ is called a fixed point of a function $f$ if it satisfies the following equation:

$$
f(x) = x
$$

We can come up with a procedure that locates fixed-points by begining with an initial guess and aplying $f$ repeatedly until the value doesn't change much.

```{r}
fixed_point <- function(f, init, tolerance = 0.00001) {
  if (abs(f(init) - init) <= tolerance) {
    return(c("fixed point" = init))
  } else {
    fixed_point(f, f(init))    ## using recursion
  }
}

fixed_point(cos, init = 5)
```

We can find an approximate solution to the equation $x = \cos x + \sin x$ using this method.

```{r}
fixed_point(function(x) cos(x) + sin(x), 1)
```

We can find the golden ratio $\phi$, which satisfies the equation $\phi = 1 + \frac{1}{\phi}$.

```{r}
fixed_point(function(x) 1 + 1/x, 1)
```

## Function Factories

```{r, message=FALSE}
library(rlang)
```

Function factories are functions that create functions.

For example:

```{r}
power <- function(exp) {
  function(x) {
    x ^ exp
  }
}

square <- power(2)
square(4)
cube <- power(3)
cube(4)
```

R has three features that, combined with each other, lead to the possibility of function factories:

1. In R, you bind a function to a name in the same way as you bind any object to a name: with the assignment operator `<-`.

2. In R, functions capture (or enclose) the environments in which they're created, that's why they're sometimes called _closures_.

3. In R, a function creates a new execution environment every time it is run. This environment is usually temporary, but here it becomes the enclosing environment of the "manufactured function".

The key idea is as follows:

>The enclosing environment of the manufactured function is an execution environment of the function factory.

### Fundamentals

Looking back at `square()` and `cube()`, note that they seem to be indistinguishable:

```{r}
square
cube
```

How does R find the correct value associated with `exp`? When `square()` executes `x ^ exp` it finds `x` in the __execution environment__ and `exp` in its __enclosing environment.__

In other words, `square()` and `cube()` have different enclosing environments, which correspond to the execution environments created each time the `power()` function is called.

>This is what makes manufactured functions behave differently from one another: names in the enclosing environment are bound to different values.

Finally, note that both environments share the same parent environment, the _global environment_ (i.e. the enclosing environment of `power()`).

```{r}
list(fn_env(square), fn_env(cube)) %>% 
  map(env_parent) 
```

___Forcing evaluations___

>Whenever you create a function factory, make sure every argument is evaluated, using `force()` as necessary if the argument is only used by the manufactured function.

As it stands, the `power()` function may fall apart because of _lazy evaluation_.

```{r}
z <- 2
square <- power(exp = z) ## exp evaluates lazily!
z <- 3                   ## the binding changes
square(2)                ## exp is only evaluated until now!
```

In other words, `x` is only evaluated (lazily) when `square()` is run, not when `power()` is run. This problem will arise everytime a binding changes in between calling the "factory" function and the "manufactured" function.

Thus, we need to __force__ `x` to be evaluated when `power()` is run the first time.

```{r}
power2 <- function(exp) {
  force(exp)
  function(x) {
    x ^ exp
  }
}

z <- 2
square <- power2(exp = z) ## exp is forced to evaluate.
z <- 3                    ## the binding changes
square(2)                 ## everything works out fine
```

__Stateful functions__

Function factories let us create functions that mantain their state across different invocations. This was not previously possible because of the "frest start principle" (i.e. "every time a function is called a new environment is created to host its execution"). 

We can do this by using the "super assignment" operator `<<-`, and by taking advantage of the fact that the enclosing environment of the manufactured functions is unique and remains constant.

The following function records how many times it has been called:

```{r}
new_counter <- function() {
  i <- 0
  
  function() {
    i <<- i + 1
    i
  }
}

counter1 <- new_counter()
counter2 <- new_counter()

replicate(5, counter1())
counter2()
map_dbl(1:5, ~ counter1())
```

>Stateful functions are best used in moderation. As soon as your function starts managing the state of multiple variables, it’s better to switch to R6.

___Garbage collection___

The garbage collector will usually get rid of any large temporary object. _However, manufactured functions hold on to the execution environment, so you’ll need to explicitly unbind any large temporary objects with `rm()`_.

```{r}
f1 <- function(n) {
  x <- runif(n)
  m <- mean(x)
  function() m
}

g1 <- f1(1e6)
lobstr::obj_size(g1)

f2 <- function(n) {
  x <- runif(n)
  m <- mean(x)
  rm(x)
  function() m
}

g2 <- f2(1e6)
lobstr::obj_size(g2)
```

___Exercises___

>Base R contains two function factories, `approxfun()` and `ecdf()`. Read their documentation and experiment to figure out what the functions do and what they return.

`approxfun()` returns a function that performs linear (or constant) interpolation.

```{r}
x <- 0:100
y <- x ^ 2

foo <- approxfun(x, y)

foo(3)
foo(5)
foo(20)
```

The `ecdf()` uses `approxfun()` to create an _empirical cumulative distribution function_.

```{r}
x <- rnorm(150)

foo <- approxfun(x = sort(x), 
                 y = cumsum(table(x)) / length(x), 
                 yleft = 0, yright = 1, f = 0,
                 method = "constant", ties = "ordered")

foo(0:3)
ecdf(x)(0:3)
pnorm(0:3)
```



>Create a function `pick()` that takes an index, `i`, as an argument and returns a function with an argument `x` that subsets `x` with `i`.

```{r}
pick <- function(i) {
  function(x) {
    x[[i]]
  }
}

map_dbl(mtcars, pick(3)) ## extracts third element of each column
```

>Create a function that creates functions that compute the i^th^ central moment of a numeric vector. You can test it by running the following code:

The $i$^th^ moment about the mean (i.e. or $i$^th^ _central moment_) of a random variable $X$ is the quantity $\mu_i = E\big[(X - E[X])^i\big]$.

```{r}
moment <- function(i) {
  force(i)
  function(x) mean((x - mean(x))^i) 
}

m1 <- moment(1)
m2 <- moment(2)

x <- runif(1000)

all.equal(m1(x), 0)                                    ## tests for "near" equality
all.equal(m2(x) * length(x) / (length(x) - 1), var(x)) ## bessel's correction
```


### Statistical Factories

__Note, all of these examples can be tackled without function factories.__

****
___The Box-Cox Transformation___

>The Box-Cox transformation (a type of power transformation) is a flexible transformation often used to transform data towards normality. It has a single parameter, $\lambda$, which controls the strength of the transformation.

Note: The square-root and log transformations are special cases of Box-Cox.

$$
y_i(\lambda) = \cases{
\frac{y_i^\lambda - 1}{\lambda} \hspace{0.5cm} \text{ if } \lambda \neq 0 \\\\
\log(y_i) \hspace{0.35cm} \text{ if } \lambda = 0
}
$$

We can express this as a simple function:

```{r}
boxcox1 <- function(x, lambda) {
  stopifnot(length(lambda) == 1)
  
  if (lambda == 0) {
    log(x)
  } else {
    (x ^ lambda - 1) / lambda
  }
}
```

Using a function factory, however, will let us explore different values of `lambda` more easily with __ggplot2__:

```{r, fig.width=5, fig.height=3}
boxcox2 <- function(lambda) {
  if (lambda == 0) {
    function(x) log(x)
  } else {
    function(x) (x ^ lambda - 1) / lambda
  }
}

stat_boxcox <- function(lambda) {
  stat_function(aes(color = as.character(lambda)), 
                fun = boxcox2(lambda), 
                size = 0.5) 
}

ggplot(data.frame(x = c(0, 5)), aes(x)) + 
  map(seq(0, 2, 0.25), stat_boxcox) +
  labs(color = expression(lambda))
```

___Bootstrap generators___

A bootstrap generator is a function that yields a new bootstrap sample every time it's called:

```{r}
boot_permute <- function(df, variable) {
  n <- nrow(df)
  force(variable)
  
  function() {
    col <- df[[variable]]
    col[sample(n, replace = TRUE)]
  }
}

boot_mpg <- boot_permute(mtcars, "mpg")

map(1:3, ~ boot_mpg()) %>% map(head)
```

___Maximum Likelihood Estimation___

The goal of MLE is to find the parameter values for a distribution, such that the observed data is "most likeley". 

_Poisson Distribution_

$$
\Pr(\mathbf x \mid \lambda) = \prod_{i = 1}^n \frac{\lambda^{x_i} e^\lambda}{x_i !}
$$

Taking the log-likelihood, and doing some extra math, we can come up with this simplified equation:

$$
\log\bigg(\Pr(\mathbf x \mid \lambda)\bigg) = \log(\lambda) \sum_{i = 1}^n x_i - n \lambda - \sum_{i = 1}^n \log(x_i!)
$$

```{r}
log_prob_poisson <- function(x, lambda) {
  n <- length(x)
  log(lambda) * sum(x) - n * lambda - sum(lfactorial(x))
}
```

What value of $\lambda$ gives us the highest `log_prob_poisson`? The likelihood function is nothing more than the probability function seen through this perspective: _we want to find the `lambda` that makes the observed `x` the most likely._ In statistics, we usually highlight this change in perspective by writing $L(\lambda; \mathbf x)$. 

In R, we can use a function factory.

```{r}
log_likelihood_poisson <- function(x) {
  n <- length(x)   ## we don't need to force(x) because length(x) evaluates x
  sum_x <- sum(x)               ## this avoids unnecessary computation later 
  sum_lfx <- sum(lfactorial(x)) ## on, when calling the next function many times
  function(lambda) {
    log(lambda)*sum_x - n*lambda - sum_lfx
  }
}

## Simulation with known lambda
x <- rpois(1e3, lambda = 8)

## Manufacture log likelihood
log_likelihood <- log_likelihood_poisson(x)

log_likelihood(lambda = 1)
log_likelihood(lambda = 5)

## Use optimise instead of trial-and-error
optimise(log_likelihood, c(0, 100), maximum = TRUE)

## Sanity check
mod <- glm(x ~ 1, family = "poisson")
exp(mod$coefficients) ## undo the link function
```

>Now, we could have solved this problem without using a function factory because `optimise()` passes `...` on to the function being optimised. That means we could use the log-probability function directly:

```{r}
optimise(log_prob_poisson, x = x, interval = c(0, 100), maximum = TRUE)
```

>The advantage of using a function factory here is fairly small, but there are two niceties:
>
>1. We can precompute some values in the factory, saving computation time in each iteration.
>
>2. The two-level design better reflects the mathematical structure of the underlying problem.
>
>__These advantages get bigger in more complex MLE problems, where you have multiple parameters and multiple data vectors__.

___Mapping over many function factories___

The following code creates many specially named power functions by iterating over a list of arguments:

```{r}
fnames <- list(
  square = 2, 
  cube = 3, 
  root = 1/2, 
  cuberoot = 1/3, 
  reciprocal = -1
)

pfuns <- purrr::map(fnames, power2)

pfuns$root(64)
```

>One downside of the current construction is that you have to prefix every function call with `funs$`. There are [four] ways to eliminate this additional syntax:

1. Use `with()`.

```{r}
with(pfuns, cube(2))
```

2. `attach()` the functions to the search path, then `detach()` when you’re done:

    >You've probably been told to avoid using `attach()`, and that's generally good advice. However, the situation is a little different to the usual because we’re attaching a list of functions, not a data frame. It’s less likely that you'll modify a function than a column in a data frame, so the some of the worst problems with `attach()` don't apply.

3. You could copy the functions to the global environment with `env_bind()`.

    ```{r}
    rlang::env_bind(globalenv(), !!!pfuns)
    ```

    >You can later unbind those same names, but there's no guarantee that they haven't been rebound in the meantime, and you might be deleting an object that someone else created.

    ```{r}
    rlang::env_unbind(globalenv(), names(pfuns))
    ```

4. Use the built-in `list2env()` function. 

    ```{r}
    list2env(pfuns, envir = globalenv())
    ```

## Function Operators

>A __function operator__ is a function that takes one (or more) functions as input and returns a function as output.

Note that they're simply _function factories_ that take functions as inputs. 

The rest of this section has three parts. We overview two useful function operators —`safely()` and `memoise()`— and then we create our own function operator.

### Capturing errors: `purrr::safely()`

>One advantage of for-loops is that if one of the iterations fails, you can still access all the results up to the failure. If you do the same thing with a functional, you get no output, making it hard to figure out where the problem lies.
>
>`purrr::safely()` is a function operator that transforms a function to turn errors into data.

Note: `safely()` uses `tryCatch()` patterns to handle errors.

>Like all function operators, safely() takes a function and returns a wrapped function which we can call as usual:

```{r}
safe_sum <- safely(sum)
```

For example:

```{r}
x <- list(
  c(0.512, 0.165, 0.717),
  "oops",
  c(0.064, 0.781, 0.427),
  c(0.890, 0.785, 0.495)
)

safe_sum(x[[1]])
safe_sum(x[[2]])
```

We can now use `safely()` with a functional.

```{r}
output <- map(x, safe_sum)
str(output)
```

The output is hard to work with since we have four lists, and each of them is a list containing the `result` and the `error`. We can make the output easier to work with by `purrr::transpose()`, which turns our output into a list of results and a list of errors:

```{r}
inside_out_output <- transpose(output)
str(inside_out_output)
```

Finally, use `map_lgl()` to extract an index that helps you track the source of your errors more easily:

```{r}
ok <- map_lgl(inside_out_output$error, is.null)

x[!ok]
```

>You can use this same technique in many different situations. For example, imagine you're fitting a generalised linear model (GLM) to a list of data frames. GLMs can sometimes fail because of optimisation problems, but you still want to be able to try to fit all the models, and later look back at those that failed.

```{r, eval=FALSE}
fit_model <- function(df) {
  glm(y ~ x1 + x2 * x3, data = df)
}

models <- transpose(map(datasets, safely(fit_model)))
ok <- map_lgl(models$error, is.null)

# which data failed to converge?
datasets[!ok]

# which models were successful?
models[ok]
```


### Caching computations: `memoise::memoise()`

>Another handy function operator is `memoise::memoise()`. It _memoises_ a function, meaning that the function will remember previous inputs and return cached results. Memoisation is an example of the classic computer science _tradeoff of memory versus speed_. A memoised function can run much faster, but because it stores all of the previous inputs and outputs, it uses more memory.

```{r}
slow_function <- function(x) {
  Sys.sleep(1)  ## time elapsed should equal 1 second
  x * runif(1, 0, 10)
}

system.time(print(slow_function(1)))
system.time(print(slow_function(1)))

# Memoise -------------------------

fast_function <- memoise::memoise(slow_function)
system.time(print(fast_function(1)))
system.time(print(fast_function(1)))  ## it remembers previous output!
```

>A relatively realistic use of memoisation is computing the Fibonacci series. The Fibonacci series is defined recursively: the first two values are defined by convention, `f(0) = 0`, `f(1) = 1`, and then `f(n) = f(n-1) + f(n-2)` (for any positive integer). A naive version is slow because, for example, `fib(10)` computes `fib(9)` and` fib(8)`, and `fib(9)` computes `fib(8)` and `fib(7)`, and so on.

```{r}
fib <- function(n) {
  if (n <= 2) return(1L)
  fib(n - 1) + fib(n - 2)  # recursivity
}

system.time(fib(23))
```

>Memoising `fib()` makes the implementation much faster because each value is computed only once:

```{r}
fast_fib <- memoise::memoise(function(n) {
  if (n <= 2) return(1L)
  fast_fib(n - 1) + fast_fib(n - 2)
})

system.time(fast_fib(23))
system.time(fast_fib(24)) 
```

>This is an example of _dynamic programming_, where a complex problem can be broken down into many overlapping subproblems, and remembering the results of a subproblem considerably improves performance.

Note, for the purposes of this problem in particular, a function with a simple for-loop will work more efficiently.

```{r}
fib_vector <- function(max) {
  x <- 1:max
  y <- vector("integer", max)
  y[1:2] <- 1
  if (max > 2) {
    for (i in 3:max) {
      y[[i]] <- y[[i - 1]] + y[[i - 2]]
    }
  }
  return(y)
}

system.time(fib_vector(10000))
```

___Exercises.___

>Base R provides a function operator in the form of `Vectorize()`. What does it do? When might you use it?

`Vectorize()` can be viewed as a wrapper for `base::mapply()` (i.e. similar to `purrr::pmap()`, but with type instabilities): it takes a function and then vectorizes it (i.e. it takes vectors as inputs and produces output vectors).

For example:

```{r, error=TRUE}
fib(1:10)
vec_fib <- Vectorize(fib)
vec_fib(1:10)
```

We could have done this instead:

```{r}
map_intize <- function(f) {
  force(f)
  function(x, ...) {
    map_int(x, f, ...)
  }
}

fib_map <- map_intize(fib)
fib_map(1:10)  ## identical to map_int(1:10, fib)


urls <- c(
  "adv-r" = "https://adv-r.hadley.nz", 
  "r4ds" = "http://r4ds.had.co.nz/"
  # and many many more
)
path <- paste(names(urls), ".html")

walk2(urls, path, download.file, quiet = TRUE)

path
```

Even though we have already seen two function operators, this is the first we have actually created one.


### Webscrapping many websites

Suppose we have a webscraper called `scrappy()`, it downloads the contents of a website and outputs a data frame. We will not delve into how to actually build one of these (I usually use a combination of [__`rvest`__](https://rvest.tidyverse.org/) and [__`dplyr`__](https://dplyr.tidyverse.org/)). Instead we will create two function operators that enhance our webscraper:

1. `delay_by()`: _add a small delay between each request to avoid hammering the server_.

2. `dot_every()`: _display a `.` every few URLs so that we know that the function is still working_.

```{r}
delay_by <- function(f, amount) {
  force(f)
  force(amount)
  
  function(...) {
    Sys.sleep(amount)
    f(...)
  }
}

## Example usage
system.time(map(1:5, delay_by(runif, 0.1)))
```

```{r}
dot_every <- function(f, n) {
  force(f)
  force(n)
  
  i <- 0
  function(...) {
    i <<- i + 1
    if(i %% n == 0) cat(".")
    f(...)
  }
}

## Example usage
walk(1:100, dot_every(runif, 5))
```

Our hypothetical `scrappy()` function could thus be enhanced as follows:

```{r}
output <- map(urls, scrappy %>% delay_by(1) %>% dot_every(5))
```

